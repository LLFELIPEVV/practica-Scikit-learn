# üìÇ HERRAMIENTAS PARA MANEJAR DATOS
import numpy as np  # Calculadora avanzada para operaciones matem√°ticas con datos
# Organizador de datos tipo Excel (tablas y an√°lisis b√°sico)
import pandas as pd

# üé® HERRAMIENTAS PARA VISUALIZACI√ìN
import matplotlib.pyplot as plt  # Pincel digital para crear gr√°ficos b√°sicos
import seaborn as sns  # Pincel profesional para gr√°ficos atractivos y detallados

# üîß HERRAMIENTAS DE MACHINE LEARNING
# Ajustadores de escala
from sklearn.preprocessing import StandardScaler, QuantileTransformer, PolynomialFeatures, OneHotEncoder
# Clasificador tipo "vecinos cercanos"
from sklearn.neighbors import KNeighborsClassifier
# Cadena de procesos autom√°ticos (escalado + modelo)
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# üé® Configuramos el estilo visual de todos los gr√°ficos
# Fondo claro y colores modernos
sns.set_theme(style="whitegrid", palette="viridis")

# üîÑ MEJORAMOS LOS DATOS PARA QUE EL MODELO ENTIENDA MEJOR:
# Escalar los datos ayuda a que el modelo funcione mejor y m√°s r√°pido
# (Como poner todas las medidas en la misma escala)

# üìÇ Cargamos nuestros datos desde un archivo CSV
df = pd.read_csv("drawndata1.csv")

# üëÄ Vistazo r√°pido a las primeras 3 filas de los datos
print("Primeras 3 filas de nuestros datos:")
df.head(3)
print("\nInformaci√≥n t√©cnica:")
df.info()  # Resumen de columnas y tipos de datos

# üéØ Separamos los datos en dos partes:
X = df[['x', 'y']].values  # Coordenadas X e Y (datos para predecir)
y = df['z'] == "a"  # Etiquetas: Verdadero/Falso seg√∫n la columna 'z'

# üìä Gr√°fico 1: Datos originales
plt.figure(figsize=(10, 6))  # Tama√±o del gr√°fico
sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette={True: "#440154", False: "#fde725"},
                edgecolor="black", alpha=0.8, s=80)
plt.title(
    "Distribuci√≥n Original de los Datos\n(Colores muestran diferentes grupos)", pad=20)
plt.xlabel("Coordenada X", labelpad=10)
plt.ylabel("Coordenada Y", labelpad=10)
plt.legend(title="Grupo A", loc="upper right")
plt.show()

# üîß Ajustamos la escala de los datos
escalador = StandardScaler()  # Creamos un ajustador de escalas
X_new = escalador.fit_transform(X)  # Transformamos los datos

# üìä Gr√°fico 2: Datos escalados
plt.figure(figsize=(10, 6))
sc = sns.scatterplot(x=X_new[:, 0], y=X_new[:, 1], hue=y,
                     palette={True: "#21918c", False: "#fde725"},
                     edgecolor="black", alpha=0.8, s=80)
plt.title("Datos despu√©s del Escalado\n(Misma informaci√≥n en nueva escala)", pad=20)
plt.xlabel("Coordenada X (Escalada)", labelpad=10)
plt.ylabel("Coordenada Y (Escalada)", labelpad=10)
plt.legend(title="Grupo A", loc="upper right")
# L√≠neas de referencia para el punto (0,0) despu√©s del escalado
plt.axhline(0, color='gray', linestyle='--', linewidth=1)
plt.axvline(0, color='gray', linestyle='--', linewidth=1)
plt.show()

# üß™ Ejemplo adicional: C√≥mo funciona el escalado
# Generamos 1000 n√∫meros aleatorios combinando dos distribuciones
datos_aleatorios = np.random.exponential(
    10, 1000) + np.random.normal(0, 1, 1000)

# üîÑ Aplicamos escalado a nuestros datos aleatorios
datos_escalados = (datos_aleatorios - np.mean(datos_aleatorios)
                   ) / np.std(datos_aleatorios)

# üìä Gr√°fico 3: Histograma mejorado
plt.figure(figsize=(10, 6))
sns.histplot(datos_escalados, bins=30, kde=True, color="#440154",
             edgecolor="white", linewidth=1.2)
plt.title(
    "Distribuci√≥n de Datos Escalados\n(L√≠nea curva muestra la tendencia)", pad=20)
plt.xlabel("Valores Estandarizados (media = 0, desviaci√≥n = 1)", labelpad=10)
plt.ylabel("Cantidad de Datos", labelpad=10)
plt.grid(axis='y', alpha=0.3)
plt.show()

# üîÑ TRANSFORMACI√ìN DE DATOS CON M√âTODO CUANTILES
# Transformaci√≥n que ordena y divide los datos en grupos iguales para mejor distribuci√≥n
# M√©todo especial para manejar valores extremos y distribuciones irregulares

# üìè APLICAMOS LA TRANSFORMACI√ìN CON 100 NIVELES DE PRECISI√ìN
# Usamos 100 divisiones para mayor precisi√≥n (como reglas de medici√≥n m√°s finas)
X_new = QuantileTransformer(n_quantiles=100).fit_transform(X)

# üìä CREAMOS UN GR√ÅFICO PROFESIONAL PARA ENTENDER LOS RESULTADOS
# Tama√±o ideal para gr√°ficos claros (10 pulgadas de ancho x 6 de alto)
plt.figure(figsize=(10, 6))

# üé® DISE√ëAMOS EL GR√ÅFICO DE PUNTOS MEJORADO
scatter = sns.scatterplot(
    x=X_new[:, 0],  # Valores transformados en el eje horizontal
    y=X_new[:, 1],  # Valores transformados en el eje vertical
    hue=y,  # Colorear puntos seg√∫n pertenezcan al Grupo A (S√≠/No)
    # Azul para el Grupo A, Amarillo para otros
    palette={True: "#2D708E", False: "#FDE725"},
    edgecolor="black",  # Borde negro para diferenciar puntos superpuestos
    alpha=0.8,  # Transparencia para ver densidad de puntos
    s=80  # Tama√±o ideal para visualizaci√≥n clara
)

# üîç PERSONALIZAMOS LA INFORMACI√ìN DEL GR√ÅFICO
# T√≠tulo descriptivo en dos l√≠neas
plt.title(
    "Datos Reorganizados con M√©todo Cuantiles\n(Mayor precisi√≥n en an√°lisis)", pad=20)
# Etiqueta eje horizontal con espacio adicional
plt.xlabel("Posici√≥n X Transformada", labelpad=10)
# Etiqueta eje vertical con espacio adicional
plt.ylabel("Posici√≥n Y Transformada", labelpad=10)
plt.legend(title="¬øPertenece al Grupo A?", loc="upper right",
           labels=['No', 'S√≠'])  # Cuadro explicativo de colores

# ‚ûï L√çNEAS GU√çA PARA MEJOR ORIENTACI√ìN
# L√≠nea horizontal central de referencia
plt.axhline(0, color='gray', linestyle='--', linewidth=1)
# L√≠nea vertical central de referencia
plt.axvline(0, color='gray', linestyle='--', linewidth=1)

# üü¶ CUADR√çCULA ESTILIZADA PARA FACILITAR LA LECTURA
plt.grid(alpha=0.3, linestyle=':')  # L√≠neas punteadas suaves como gu√≠a visual

plt.show()  # Mostrar el gr√°fico final


# üéØ FUNCI√ìN PARA COMPARAR M√âTODOS DE ESCALADO
def plot_output(scaler):
    """
    Crea una comparaci√≥n visual de 3 elementos:
    1. Datos originales
    2. Datos transformados
    3. Predicciones del modelo

    Par√°metro:
    scaler: Herramienta para ajustar la escala de los datos
    """

    # üîß Configuramos el proceso de an√°lisis (escalado + modelo predictivo)
    pipe = Pipeline([
        ("scale", scaler),  # Paso 1: Ajustar escala
        # Paso 2: Modelo de predicci√≥n
        ("model", KNeighborsClassifier(n_neighbors=20, weights='distance'))
    ])

    # üß† Entrenamos el modelo con nuestros datos
    pipe.fit(X, y)

    # üîÆ Generamos predicciones para todos los puntos
    # pred = pipe.predict(X)

    # üìä Configuramos el lienzo para 3 gr√°ficos
    plt.figure(figsize=(15, 5))
    sns.set_theme(style="whitegrid", palette="viridis")  # Estilo profesional

    # üñºÔ∏è Gr√°fico 1: Datos Originales
    plt.subplot(1, 3, 1)
    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y,
                    palette="deep", edgecolor="black")
    plt.title("Datos Originales\n(Estado natural de los datos)")
    plt.xlabel("Caracter√≠stica 1")
    plt.ylabel("Caracter√≠stica 2")
    plt.legend(title="Categor√≠a")

    # üñºÔ∏è Gr√°fico 2: Datos Transformados
    plt.subplot(1, 3, 2)
    X_tfm = scaler.transform(X)
    sns.scatterplot(x=X_tfm[:, 0], y=X_tfm[:, 1], hue=y,
                    palette="deep", edgecolor="black")
    plt.title(f"Datos Transformados\n({scaler.__class__.__name__})")
    plt.xlabel("Caracter√≠stica 1 Escalada")
    plt.ylabel("Caracter√≠stica 2 Escalada")
    plt.legend().remove()

    # üñºÔ∏è Gr√°fico 3: Superficie de Decisi√≥n
    plt.subplot(1, 3, 3)
    # Generamos 5000 puntos aleatorios para mapear predicciones
    X_new = np.concatenate([
        np.random.uniform(0, X[:, 0].max(), (5000, 1)),
        np.random.uniform(0, X[:, 1].max(), (5000, 1))
    ], axis=1)

    # Calculamos probabilidades de predicci√≥n
    y_proba = pipe.predict_proba(X_new)

    # Graficamos el mapa de predicciones con degradado de color
    sns.scatterplot(x=X_new[:, 0], y=X_new[:, 1], hue=y_proba[:, 1],
                    palette="coolwarm", edgecolor="none", alpha=0.8)
    plt.title("Superficie de Decisi√≥n\n(Probabilidad de pertenencia a clase)")
    plt.xlabel("Caracter√≠stica 1")
    plt.ylabel("Caracter√≠stica 2")
    plt.legend().remove()

    plt.tight_layout()
    plt.show()


# üîÑ PRIMER EXPERIMENTO: Escalado Est√°ndar
print("=== Resultados con Escalado Est√°ndar ===")
scaler_std = StandardScaler()
plot_output(scaler=scaler_std)

# üîÑ SEGUNDO EXPERIMENTO: Escalado por Cuantiles
print("\n=== Resultados con Escalado por Cuantiles ===")
scaler_qt = QuantileTransformer(n_quantiles=100)
plot_output(scaler=scaler_qt)

# üìÇ CARGAMOS Y PREPARAMOS LOS DATOS
# Abrimos nuestro archivo de datos (como un libro de Excel digital)
df = pd.read_csv("drawndata2.csv")

# üéØ SEPARAMOS LA INFORMACI√ìN:
X = df[['x', 'y']].values  # Usamos las columnas x e y como coordenadas
y = df['z'] == "a"  # Creamos etiquetas: True para 'a', False para otros valores

# üé® CONFIGURACI√ìN VISUAL
# Fondo claro y colores vibrantes
sns.set_theme(style="whitegrid", palette="bright")

# üìä GR√ÅFICO 1: DATOS ORIGINALES
plt.figure(figsize=(10, 6))  # Tama√±o profesional para mejor visualizaci√≥n
sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette={True: "#FF6B6B", False: "#4ECDC4"},
                edgecolor="black", s=80, alpha=0.9)
plt.title("Distribuci√≥n Original de los Datos",
          pad=15, fontsize=14)  # T√≠tulo claro
plt.xlabel("Coordenada X", labelpad=10)  # Etiqueta descriptiva
plt.ylabel("Coordenada Y", labelpad=10)
plt.legend(title="Pertenece al Grupo A", labels=[
           'No', 'S√≠'])  # Leyenda explicativa
plt.show()

# üîß CONFIGURACI√ìN DEL MODELO INTELIGENTE
pipe = Pipeline([
    # Creamos combinaciones de caracter√≠sticas (ej: x¬≤, y¬≤, xy)
    ("scale", PolynomialFeatures()),
    ("model", LogisticRegression())  # Modelo para encontrar patrones complejos
])

# üéì ENTRENAMOS EL MODELO
# El modelo aprende patrones y hace predicciones
pred = pipe.fit(X, y).predict(X)

# üìà GR√ÅFICO 2: PREDICCIONES DEL MODELO
plt.figure(figsize=(10, 6))
scatter = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=pred, palette={True: "#FF6B6B", False: "#4ECDC4"},
                          edgecolor="black", s=80, alpha=0.9)
plt.title("L√≠mites de Decisi√≥n del Modelo", pad=15, fontsize=14)
plt.xlabel("Coordenada X", labelpad=10)
plt.ylabel("Coordenada Y", labelpad=10)

# üñåÔ∏è Personalizaci√≥n adicional
# T√≠tulo de leyenda actualizado
scatter.legend_.set_title("Predicci√≥n del Modelo")
for text in scatter.legend_.texts:  # Cambiamos los textos de la leyenda
    text.set_text("Grupo A" if text.get_text() == "True" else "Otros grupos")

plt.show()

# üéØ EJEMPLO DE DATOS CATEG√ìRICOS
# Creamos un arreglo de categor√≠as (texto) que queremos convertir a n√∫meros
arr = np.array(["low", "low", "high", "medium"]).reshape(-1,
                                                         1)  # Formato requerido: matriz de 1 columna
print("üìã Datos categ√≥ricos originales:")
print(arr)  # Mostramos c√≥mo se ven los datos antes de la transformaci√≥n

# üîß CONFIGURAMOS EL CODIFICADOR ONE-HOT
# OneHotEncoder convierte categor√≠as en vectores num√©ricos (ideal para modelos de ML)
enc = OneHotEncoder(sparse_output=False,  # Devuelve una matriz densa (f√°cil de leer)
                    handle_unknown='ignore')  # Ignora categor√≠as no vistas durante el entrenamiento

# üõ†Ô∏è TRANSFORMAMOS LOS DATOS
# Aprendemos las categor√≠as y las convertimos
encoded_arr = enc.fit_transform(arr)
print("\nüî¢ Datos codificados (One-Hot Encoding):")
print(encoded_arr)  # Mostramos el resultado de la transformaci√≥n

# üí° EXPLICACI√ìN DEL RESULTADO:
# Cada categor√≠a se convierte en un vector binario:
# - "low" ‚Üí [1, 0, 0]
# - "medium" ‚Üí [0, 1, 0]
# - "high" ‚Üí [0, 0, 1]

# üß™ PRUEBA CON UNA CATEGOR√çA NUEVA
# Simulamos una categor√≠a que no estaba en los datos originales
# Transformamos "zero" usando el codificador ya entrenado
new_category = enc.transform([["zero"]])
print("\n‚ö†Ô∏è Resultado para categor√≠a nueva ('zero'):")
print(new_category)  # Muestra c√≥mo se manejan categor√≠as desconocidas

# üìù NOTA IMPORTANTE:
# - handle_unknown='ignore' hace que categor√≠as nuevas se codifiquen como [0, 0, 0]
# - Esto evita errores cuando el modelo encuentra datos no vistos durante el entrenamiento
