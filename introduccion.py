# üì¶ Importamos herramientas para trabajar con datos y gr√°ficos
import matplotlib.pyplot as plt  # Para dibujar gr√°ficos
import seaborn as sns  # Para hacer gr√°ficos m√°s claros y profesionales
import pandas as pd  # Para organizar datos en tablas como Excel

# üõ†Ô∏è HERRAMIENTAS DE APRENDIZAJE AUTOM√ÅTICO:
from sklearn.datasets import load_diabetes  # Datos de pacientes con diabetes
# Modelo de predicci√≥n tipo "vecinos cercanos"
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler  # Ajustador de escalas num√©ricas
from sklearn.pipeline import Pipeline  # Cadena de procesos autom√°ticos
from sklearn.model_selection import GridSearchCV  # Buscador de mejores ajustes

# üìù PASOS B√ÅSICOS DEL APRENDIZAJE AUTOM√ÅTICO:
# 1. Obtener datos (informaci√≥n de entrada).
# 2. Separar los datos en caracter√≠sticas (X) y lo que queremos predecir (y).
# 3. Crear un modelo que pueda aprender de los datos.
# 4. Entrenar el modelo con los datos.
# 5. Usar el modelo para hacer predicciones.

# üìÇ CARGAMOS LOS DATOS DE DIABETES
# X = Caracter√≠sticas de los pacientes (edad, peso, an√°lisis de sangre, etc.)
# y = Progresi√≥n de la diabetes (lo que queremos predecir)
X, y = load_diabetes(return_X_y=True)

# üñ®Ô∏è MOSTRAMOS LOS DATOS CARGADOS
print("üìä Datos de entrada (caracter√≠sticas):")
print(X)  # Tabla con n√∫meros que representan las caracter√≠sticas
print("\nüéØ Lo que queremos predecir:")
print(y)  # Lista de valores que representan la progresi√≥n de la diabetes

# üé® CONFIGURAMOS EL ESTILO DE LOS GR√ÅFICOS
# Fondo con cuadr√≠cula para mejor visualizaci√≥n
sns.set_theme(style="whitegrid")

# --- üîç PRIMER EXPERIMENTO: Sin normalizar los datos ---
# Creamos un modelo b√°sico de predicci√≥n (KNN: Vecinos m√°s cercanos)
modelo_basico = KNeighborsRegressor()

# üéì ENTRENAMOS EL MODELO CON LOS DATOS
modelo_basico.fit(X, y)

# üîÆ HACEMOS PREDICCIONES CON EL MODELO B√ÅSICO
predicciones_sin_ajuste = modelo_basico.predict(X)

# üìä CREAMOS GR√ÅFICOS PARA COMPARAR PREDICCIONES Y VALORES REALES
plt.figure(figsize=(15, 5))  # Tama√±o del lienzo para los gr√°ficos

# üìà GR√ÅFICO 1: Predicciones sin normalizar los datos
plt.subplot(1, 3, 1)  # Posici√≥n 1 en una fila de 3 gr√°ficos
sns.scatterplot(x=predicciones_sin_ajuste, y=y,
                alpha=0.6, color='blue')  # Puntos azules
plt.plot([min(y), max(y)], [min(y), max(y)], color='red',
         linestyle='--')  # L√≠nea de referencia ideal
plt.xlabel("Predicciones (sin normalizar)")  # Etiqueta del eje X
plt.ylabel("Valores reales")  # Etiqueta del eje Y
plt.title("Predicciones sin normalizaci√≥n")  # T√≠tulo del gr√°fico

# --- üß™ SEGUNDO EXPERIMENTO: Normalizando los datos ---
# Mejoramos el proceso: primero normalizamos los datos, luego hacemos predicciones
tuberia_mejorada = Pipeline([
    ("escalador", StandardScaler()),  # Paso 1: Normalizar los datos
    ("modelo", KNeighborsRegressor(n_neighbors=1))  # Paso 2: Predecir con KNN
])

# üéì ENTRENAMOS EL MODELO MEJORADO
tuberia_mejorada.fit(X, y)

# üîÆ HACEMOS PREDICCIONES CON EL MODELO MEJORADO
predicciones_con_ajuste = tuberia_mejorada.predict(X)

# üìà GR√ÅFICO 2: Predicciones con datos normalizados
plt.subplot(1, 3, 2)  # Posici√≥n 2 en una fila de 3 gr√°ficos
sns.scatterplot(x=predicciones_con_ajuste, y=y,
                alpha=0.6, color='green')  # Puntos verdes
plt.plot([min(y), max(y)], [min(y), max(y)], color='red',
         linestyle='--')  # L√≠nea de referencia ideal
plt.xlabel("Predicciones (con normalizaci√≥n)")
plt.ylabel("Valores reales")
plt.title("Predicciones con normalizaci√≥n")

# --- üöÄ TERCER EXPERIMENTO: Optimizando el modelo ---
# Buscamos autom√°ticamente la mejor configuraci√≥n para el modelo
print("\n‚öôÔ∏è Par√°metros que podemos ajustar:", tuberia_mejorada.get_params())

# Configuramos el buscador de mejores ajustes (GridSearchCV)
buscador_optimizador = GridSearchCV(
    estimator=tuberia_mejorada,  # Usamos el modelo mejorado
    # Probamos diferentes valores
    param_grid={"modelo__n_neighbors": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
    cv=3  # Validaci√≥n cruzada: prueba cada opci√≥n 3 veces
)

# üéì ENTRENAMOS EL BUSCADOR PARA ENCONTRAR LA MEJOR CONFIGURACI√ìN
buscador_optimizador.fit(X, y)

# üíæ GUARDAMOS LOS RESULTADOS EN UN ARCHIVO CSV
resultados_experimentos = pd.DataFrame(buscador_optimizador.cv_results_)
resultados_experimentos.to_csv('resultados_optimizacion.csv', index=False)

# üèÜ MOSTRAMOS LOS MEJORES PAR√ÅMETROS ENCONTRADOS
print(f"‚úÖ Mejores par√°metros: {buscador_optimizador.best_params_}")

# üîß ACTUALIZAMOS EL MODELO CON LOS MEJORES PAR√ÅMETROS
tuberia_mejorada.set_params(**buscador_optimizador.best_params_)
tuberia_mejorada.fit(X, y)  # Reentrenamos el modelo con la mejor configuraci√≥n

# üîÆ HACEMOS PREDICCIONES CON EL MODELO OPTIMIZADO
predicciones_optimizadas = tuberia_mejorada.predict(X)

# üìà GR√ÅFICO 3: Predicciones con la mejor configuraci√≥n
plt.subplot(1, 3, 3)  # Posici√≥n 3 en una fila de 3 gr√°ficos
sns.scatterplot(x=predicciones_optimizadas, y=y, alpha=0.6,
                color='purple')  # Puntos morados
plt.plot([min(y), max(y)], [min(y), max(y)], color='red',
         linestyle='--')  # L√≠nea de referencia ideal
plt.xlabel("Predicciones (mejor configuraci√≥n)")
plt.ylabel("Valores reales")
plt.title("Predicciones optimizadas")

# üñºÔ∏è MOSTRAMOS TODOS LOS GR√ÅFICOS JUNTOS
plt.tight_layout()  # Ajustamos el espacio entre gr√°ficos
plt.show()  # Abrimos la ventana con los gr√°ficos

# üìö INFORMACI√ìN ADICIONAL SOBRE LOS DATOS
diabetes = load_diabetes()
print("\nDescripci√≥n t√©cnica del conjunto de datos:")
print(diabetes.DESCR)  # Explicaci√≥n m√©dica de las variables
